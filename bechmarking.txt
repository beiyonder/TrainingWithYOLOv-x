Accuracy: Accuracy measures the overall correctness of a model's predictions. It is the ratio of the number of correct predictions to the total number of predictions. While accuracy is a widely used metric, it may not be suitable for imbalanced datasets.

Precision: Precision is the proportion of true positive predictions (correctly identified positive instances) out of the total predicted positive instances. It quantifies the model's ability to minimize false positives.

Recall: Recall, also known as sensitivity or true positive rate, measures the proportion of true positive predictions out of the actual positive instances. It quantifies the model's ability to minimize false negatives.

F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of a model's performance, particularly when precision and recall have unequal importance.

Mean Average Precision (mAP): mAP is a popular metric for evaluating object detection models. It measures the precision and recall trade-off at various thresholds of detection confidence. The mAP is calculated by averaging the average precision (AP) across multiple classes or object categories.

Intersection over Union (IoU): IoU is a measure used in object detection and instance segmentation tasks. It quantifies the overlap between the predicted bounding box or segmentation mask and the ground truth. It is calculated as the ratio of the intersection area to the union area between the predicted and ground truth regions.

Top-k Accuracy: Top-k accuracy evaluates how often the correct label appears within the top-k predicted labels. It is useful when considering models that can produce multiple predictions or ranking tasks.

Receiver Operating Characteristic (ROC) curve: The ROC curve illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate (1-specificity) at various classification thresholds. It helps assess the performance of binary classification models and allows comparison between different models.

Area Under the ROC Curve (AUC-ROC): AUC-ROC summarizes the performance of a binary classification model across all possible classification thresholds. It provides a single value indicating the model's ability to rank examples correctly and is useful for evaluating models with imbalanced datasets.

Mean Average Precision at different Intersection over Union thresholds (mAP@[IoU]): In tasks like instance segmentation, where accurate localization is important, mAP is often computed at different IoU thresholds (e.g., 0.5, 0.75, 0.9). This provides a more detailed evaluation of a model's performance at different levels of object overlap.

Mean Squared Error (MSE): MSE is a regression metric that measures the average squared difference between the predicted and actual values. It is commonly used to evaluate models that predict continuous numerical values.

Root Mean Squared Error (RMSE): RMSE is the square root of the mean squared error. It provides a more interpretable metric as it is in the same units as the target variable.

Mean Absolute Error (MAE): MAE is a regression metric that measures the average absolute difference between the predicted and actual values. It is less sensitive to outliers compared to MSE.

R-squared (RÂ²) or Coefficient of Determination: R-squared measures the proportion of the variance in the dependent variable that can be explained by the independent variables in a regression model. It ranges from 0 to 1, with higher values indicating a better fit.

Log Loss: Log loss, also known as cross-entropy loss, is commonly used in binary and multiclass classification problems. It measures the performance of a model by penalizing incorrect predictions and is particularly useful when dealing with probabilistic predictions.

Mean Average Error (MAE): MAE is a metric used to evaluate models that generate sequence predictions. It measures the average absolute difference between the predicted and actual sequence values.

BLEU Score: BLEU (Bilingual Evaluation Understudy) is a metric commonly used to evaluate the quality of machine-generated translations. It measures the overlap between the predicted and reference translations based on n-gram matching.

Perplexity: Perplexity is a metric commonly used to evaluate language models. It quantifies how well a language model predicts a sample text. Lower perplexity values indicate better performance.

Speedup: Speedup measures the performance improvement achieved by a deep learning model or framework when compared to a baseline or reference model. It quantifies the reduction in training or inference time.

Parameter Count: Parameter count refers to the total number of learnable parameters in a deep learning model. It is often used to assess the model's complexity and memory requirements.